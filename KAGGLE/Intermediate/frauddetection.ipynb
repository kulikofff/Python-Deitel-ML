{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-14T14:30:46.766563Z","iopub.execute_input":"2022-11-14T14:30:46.766920Z","iopub.status.idle":"2022-11-14T14:30:46.779615Z","shell.execute_reply.started":"2022-11-14T14:30:46.766878Z","shell.execute_reply":"2022-11-14T14:30:46.779015Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"/kaggle/input/credit-card-fraud/card_transdata.csv\n/kaggle/input/card-transdata-test/card_transdata_test.csv\n/kaggle/input/fraud1test/card_transdata_test_fraud.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Read the data\n#The source of dataset - https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud\nX = pd.read_csv('../input/credit-card-fraud/card_transdata.csv')\nX_test_full = pd.read_csv('../input/card-transdata-test/card_transdata_test.csv')\n\nprint('X and X_test before data preparation:')\nprint(X.shape)\nprint(X_test_full.shape)\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['fraud'], inplace=True)\ny = X.fraud              \nX.drop(['fraud'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n###For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  \n###For this reason, we typically will only one-hot encode columns with relatively low cardinality.  \n###Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding.\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n\nprint()\nprint('X and X_test after data preparation:')\nprint(X_train.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T14:30:46.780824Z","iopub.execute_input":"2022-11-14T14:30:46.781744Z","iopub.status.idle":"2022-11-14T14:30:47.906734Z","shell.execute_reply.started":"2022-11-14T14:30:46.781671Z","shell.execute_reply":"2022-11-14T14:30:47.905558Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"X and X_test before data preparation:\n(1000000, 8)\n(1156, 8)\n\nX and X_test before data preparation:\n(800000, 7)\n(1000000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Create pipeline:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_cols),\n        ('cat', categorical_transformer, low_cardinality_cols)\n    ])\n\n# Define model\n#model = RandomForestRegressor(n_estimators=100, random_state=0)\n#model = XGBRegressor(random_state=0)\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))\n","metadata":{"execution":{"iopub.status.busy":"2022-11-14T14:30:47.908002Z","iopub.execute_input":"2022-11-14T14:30:47.908385Z","iopub.status.idle":"2022-11-14T14:35:50.505750Z","shell.execute_reply.started":"2022-11-14T14:30:47.908361Z","shell.execute_reply":"2022-11-14T14:35:50.505102Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"MAE: 9.777023159497133e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"print(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'fraud': preds_test})\noutput.to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T14:40:52.979512Z","iopub.execute_input":"2022-11-14T14:40:52.979820Z","iopub.status.idle":"2022-11-14T14:40:52.987422Z","shell.execute_reply.started":"2022-11-14T14:40:52.979796Z","shell.execute_reply":"2022-11-14T14:40:52.986109Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"x1 = pd.read_csv('../input/inputfraud1test/card_transdata_test_fraud1.csv')\npreds1 = clf.predict(x1)\nprint(f'Prediction of one real transaction: {preds1}')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T16:25:46.083846Z","iopub.execute_input":"2022-11-14T16:25:46.084233Z","iopub.status.idle":"2022-11-14T16:25:46.119769Z","shell.execute_reply.started":"2022-11-14T16:25:46.084206Z","shell.execute_reply":"2022-11-14T16:25:46.119085Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"Prediction of one real transaction: [1.0013679]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Try and compare different models\n\nestimators = {\n    'XGBRegressor1': XGBRegressor(n_estimators=200, learning_rate=0.05),\n    'XGBRegressor2': XGBRegressor(n_estimators=100, learning_rate=1),\n    'XGBRegressor3': XGBRegressor(random_state=0),\n    'RandomForestRegressor': RandomForestRegressor(n_estimators=100, random_state=0),\n    'XGBRegressor3': XGBRegressor(n_estimators=1000, learning_rate=0.05)\n}","metadata":{"execution":{"iopub.status.busy":"2022-11-14T15:37:49.408741Z","iopub.execute_input":"2022-11-14T15:37:49.409070Z","iopub.status.idle":"2022-11-14T15:37:49.416001Z","shell.execute_reply.started":"2022-11-14T15:37:49.409045Z","shell.execute_reply":"2022-11-14T15:37:49.414479Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfor estimator_name, estimator_object in estimators.items():\n    # Bundle preprocessing and modeling code in a pipeline\n    clf = Pipeline(steps=[('preprocessor', preprocessor), ('model', estimator_object)])\n    # Preprocessing of training data, fit model \n    clf.fit(X_train, y_train)\n    # Preprocessing of validation data, get predictions\n    preds = clf.predict(X_valid)\n    \n    print('MAE scoring')\n    MAE = mean_absolute_error(y_valid, preds)\n    print(f'{estimator_name:>16}: MAE: {MAE:.6f}')\n\n    print('Cross_val_score')\n    scores = -1 * cross_val_score(clf, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n    print(\"Average MAE score (CV):\", scores.mean())\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}