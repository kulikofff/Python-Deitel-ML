<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>ConvNetJS Gradcheck</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- <link rel="stylesheet" href="style.css"> -->
</head>

<script src="../demo/jquery-1.8.3.min.js"></script>
<script src="../build/convnet.js"></script>

<script>

var dv = 0.000001;

function almosteq(a, b, txt) {
  var rel_error1 = (a-b)*(a-b)/(a*a);
  var rel_error2 = (a-b)*(a-b)/(b*b);
  var rel_error = Math.max(rel_error1, rel_error2);
  if(a===b) rel_error=0;
  var f = true;
  if(typeof a==='undefined' || isNaN(a) || isNaN(b) || typeof b ==='undefined') { rel_error = Infinity; };
  var re = '(rel_error = ' + rel_error + ')';
  var msg = 'PASS ' + re;
  if(rel_error > 0.001) { msg = 'WARN ' + re; f = false;}
  if(rel_error > 0.02) { msg = 'FAIL ' + re; f = false;}
  if(!f)
  console.log(msg + ' (' + txt + '): ' + a + ' vs. ' + b);
  return f;
}

var gradcheck_softmax = function() {

  // random example
  var sx = 1;
  var sy = 1;
  var D = 5;
  var x = new convnetjs.Vol(sx, sy, D);
  var y = 1; // ground truth pretend label

  var C = 3; // num classes
  
  // set up a network
  var layer_defs = [];
  layer_defs.push({type:'input', out_sx:sx, out_sy:sy, out_depth:D});
  layer_defs.push({type:'softmax', num_classes:C});

  net = new convnetjs.Net();
  net.makeLayers(layer_defs);

  // feed forward test
  A = net.forward(x);
  var S = net.layers[1]; // fully conn layer created internally
  net.backward(y); // computes the gradient

  // evaluate and check numerical gradient (for both weights and biases)
  var match = true;
  for(var k=0;k<C;k++) {
    for(var d=0;d<=D;d++) {
      var v;
      if(d==D) v = S.biases.w[k];
      else v = S.filters[k].w[d];
      var v1 = v - dv;
      var v2 = v + dv;
      if(d==D) S.biases.w[k] = v1;
      else S.filters[k].w[d] = v1;
      var A1 = net.forward(x);
      var L1 = -Math.log(A1.w[y]);
      if(d==D) S.biases.w[k] = v2;
      else S.filters[k].w[d] = v2;
      var A2 = net.forward(x);
      var L2 = -Math.log(A2.w[y]);
      var DL = (L2-L1)/(2*dv);

      if(d==D) S.biases.w[k] = v; //reset
      else S.filters[k].w[d] = v;

      if(d==D) {
        almosteq(DL, S.biases.dw[k], 'SOFTMAX gradcheck bias');
      } else {
        almosteq(DL, S.filters[k].dw[d], 'SOFTMAX gradcheck');
      }
    }
  }

  // evaluate and check numerical gradient wrt data
  for(var i=0;i<x.w.length;i++) {
    var v = x.w[i];
    var v1 = v - dv;
    var v2 = v + dv;
    x.w[i] = v1;
    var A1 = net.forward(x);
    var L1 = -Math.log(A1.w[y]);
    x.w[i] = v2;
    var A2 = net.forward(x);
    var L2 = -Math.log(A2.w[y]);
    var DL = (L2-L1)/(2*dv);

    x.w[i] = v; // reset

    match = match && almosteq(DL, x.dw[i], 'SOFTMAX data gradcheck');
  }

  return match;
}

var gradcheck_lrn = function() {

  // random example
  var sx = 3; 
  var sy = 3;
  var x = new convnetjs.Vol(sx, sy, 15);
  var y = 1; // ground truth pretend label

  // set up a network
  var layer_defs = [];
  layer_defs.push({type:'input', out_sx:x.sx, out_sy:x.sy, out_depth:x.depth});
  layer_defs.push({type:'lrn', k:1,n:3,alpha:0.1,beta:0.75});
  layer_defs.push({type:'softmax', num_classes:4});

  net = new convnetjs.Net();
  net.makeLayers(layer_defs);

  // feed forward test
  A = net.forward(x);

  // evaluate analytical gradient
  net.backward(y); // computes gradients in layers

  // evaluate grad wrt input
  for(var i=0;i<x.w.length;i++) {
    var v = x.w[i];
    var v1 = v - dv;
    var v2 = v + dv;

    x.w[i] = v1;
    var A1 = net.forward(x);
    var L1 = -Math.log(A1.w[y]);

    x.w[i] = v2;
    var A2 = net.forward(x);
    var L2 = -Math.log(A2.w[y]);

    var DL = (L2-L1)/(2*dv);
    x.w[i] = v;

    almosteq(DL, x.dw[i], 'LRN data gradcheck ' + i);
  }
}

var gradcheck_net = function() {

  // random example
  var sx = 8; 
  var sy = 8;
  var x = new convnetjs.Vol(sx, sy, 4);
  var y = 1; // ground truth pretend label

  // set up a network
  var layer_defs = [];
  layer_defs.push({type:'input', out_sx:sx, out_sy:sx, out_depth:4});
  layer_defs.push({type:'conv', sx:3, filters:5, stride:1});
  layer_defs.push({type:'pool', sx:2, stride:2});
  layer_defs.push({type:'conv', sx:2, filters:4, stride:1, activation:'relu'});
  layer_defs.push({type:'pool', sx:2, stride:2});
  layer_defs.push({type:'softmax', num_classes:4});

  net = new convnetjs.Net();
  net.makeLayers(layer_defs);

  // feed forward test
  A = net.forward(x);

  // evaluate analytical gradient
  net.backward(y); // computes gradients in layers

  // evaluate and check numerical gradient wrt data in input layer
  // which is almost sufficient since that's a function of gradients
  // all the way up
  for(var i=0;i<x.w.length;i++) {
    var v = x.w[i];
    var v1 = v - dv;
    var v2 = v + dv;

    x.w[i] = v1;
    var A1 = net.forward(x);
    var L1 = -Math.log(A1.w[y]);

    x.w[i] = v2;
    var A2 = net.forward(x);
    var L2 = -Math.log(A2.w[y]);

    var DL = (L2-L1)/(2*dv);
    x.w[i] = v;

    almosteq(DL, x.dw[i], 'NET data gradcheck ' + i);
  }
}

// lightweight test
$(window).load(function() {
  // even if we fail sometimes it might be okay because 
  // the maxes and relus create hard discontinuities in
  // the objective function, so finite differences arent
  // exactly appropriate approximation
  var elt = document.getElementById("out");
  gradcheck_softmax();
  gradcheck_lrn();
  gradcheck_net();
});

</script>

<body>
  <div id="out"></div>
</body>
</html>



